{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import math\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "EPOCHS = 3 # originally 3\n",
    "BATCH_SIZE = 32 # originally 32\n",
    "PAD_ID = 1\n",
    "SEED = 88888\n",
    "LABEL_SMOOTHING = 0.1\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask[k,:len(enc.ids)+3] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+2] = 1\n",
    "        end_tokens[k,toks[-1]+2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "def save_weights(model, dst_fn):\n",
    "    weights = model.get_weights()\n",
    "    with open(dst_fn, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n",
    "\n",
    "def load_weights(model, weight_fn):\n",
    "    with open(weight_fn, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    model.set_weights(weights)\n",
    "    return model\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    # adjust the targets for sequence bucketing\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
    "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.GRU(units = 64, return_sequences = True)(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.GRU(units = 64, return_sequences = True)(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    # this is required as `model.predict` needs a fixed size!\n",
    "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    \n",
    "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
    "    return model, padded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "21984/21984 [==============================] - 135s 6ms/sample - loss: 2.8815 - activation_loss: 1.4615 - activation_1_loss: 1.4201 - val_loss: 2.5065 - val_activation_loss: 1.2751 - val_activation_1_loss: 1.2325\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 2/2\n",
      "21984/21984 [==============================] - 106s 5ms/sample - loss: 2.5110 - activation_loss: 1.2773 - activation_1_loss: 1.2336 - val_loss: 2.5128 - val_activation_loss: 1.2778 - val_activation_1_loss: 1.2360\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 3/3\n",
      "21984/21984 [==============================] - 106s 5ms/sample - loss: 2.3990 - activation_loss: 1.2210 - activation_1_loss: 1.1780 - val_loss: 2.5261 - val_activation_loss: 1.2775 - val_activation_1_loss: 1.2497\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 17s 3ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 9s 2ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.7072281911680897\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "21985/21985 [==============================] - 131s 6ms/sample - loss: 2.8420 - activation_loss: 1.4353 - activation_1_loss: 1.4039 - val_loss: 2.4889 - val_activation_loss: 1.2742 - val_activation_1_loss: 1.2162\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 2/2\n",
      "21985/21985 [==============================] - 107s 5ms/sample - loss: 2.4976 - activation_loss: 1.2716 - activation_1_loss: 1.2268 - val_loss: 2.4697 - val_activation_loss: 1.2672 - val_activation_1_loss: 1.2038\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 3/3\n",
      "21985/21985 [==============================] - 113s 5ms/sample - loss: 2.4203 - activation_loss: 1.2275 - activation_1_loss: 1.1909 - val_loss: 2.4968 - val_activation_loss: 1.2694 - val_activation_1_loss: 1.2287\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 17s 3ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 8s 2ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7091563990692928\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "21985/21985 [==============================] - 131s 6ms/sample - loss: 2.8974 - activation_loss: 1.4594 - activation_1_loss: 1.4354 - val_loss: 2.5513 - val_activation_loss: 1.2996 - val_activation_1_loss: 1.2529\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 2/2\n",
      "21985/21985 [==============================] - 116s 5ms/sample - loss: 2.4353 - activation_loss: 1.2368 - activation_1_loss: 1.1971 - val_loss: 2.5335 - val_activation_loss: 1.2951 - val_activation_1_loss: 1.2401\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 17s 3ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 8s 2ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.7028112509815825\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "21985/21985 [==============================] - 138s 6ms/sample - loss: 2.9418 - activation_loss: 1.4818 - activation_1_loss: 1.4623 - val_loss: 2.5327 - val_activation_loss: 1.2826 - val_activation_1_loss: 1.2515\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 2/2\n",
      "21985/21985 [==============================] - 109s 5ms/sample - loss: 2.5336 - activation_loss: 1.2903 - activation_1_loss: 1.2494 - val_loss: 2.4972 - val_activation_loss: 1.2706 - val_activation_1_loss: 1.2279\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 3/3\n",
      "21985/21985 [==============================] - 113s 5ms/sample - loss: 2.4609 - activation_loss: 1.2505 - activation_1_loss: 1.2088 - val_loss: 2.4833 - val_activation_loss: 1.2687 - val_activation_1_loss: 1.2160\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 17s 3ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 9s 3ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7049758132200019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model, padded_model = build_model()\n",
    "        \n",
    "    #sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "    #    save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n",
    "    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n",
    "    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n",
    "    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n",
    "    # sort the validation data\n",
    "    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n",
    "    inpV = [arr[shuffleV] for arr in inpV]\n",
    "    targetV = [arr[shuffleV] for arr in targetV]\n",
    "    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # sort and shuffle: We add random numbers to not have the same order in each epoch\n",
    "        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n",
    "        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n",
    "        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n",
    "        batch_inds = np.random.permutation(num_batches)\n",
    "        shuffleT_ = []\n",
    "        for batch_ind in batch_inds:\n",
    "            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n",
    "        shuffleT = np.concatenate(shuffleT_)\n",
    "        # reorder the input data\n",
    "        inpT = [arr[shuffleT] for arr in inpT]\n",
    "        targetT = [arr[shuffleT] for arr in targetT]\n",
    "        model.fit(inpT, targetT, \n",
    "            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n",
    "            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n",
    "        save_weights(model, weight_fn)\n",
    "\n",
    "    print('Loading model...')\n",
    "    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    load_weights(model, weight_fn)\n",
    "\n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>12005b65fc</td>\n",
       "      <td>Waiting for my turn on wii fit gym closed</td>\n",
       "      <td>neutral</td>\n",
       "      <td>waiting for my turn on wii fit gym closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>bcf13877f7</td>\n",
       "      <td>Good morning everyone</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>575e4a89fe</td>\n",
       "      <td>tts ridiculously sweet of you</td>\n",
       "      <td>positive</td>\n",
       "      <td>ridiculously sweet of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>a0b1828b67</td>\n",
       "      <td>'Brides a la mode' pow wow first thing this morning   Th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>472c3e2c41</td>\n",
       "      <td>Getting somewhere with my first 'real' KiokuDB and catal...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>getting somewhere with my first 'real' kiokudb and cata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>ce71d002ec</td>\n",
       "      <td>Mommas day is may 10th! Don`t forget to do something nic...</td>\n",
       "      <td>positive</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>8db4aaef4a</td>\n",
       "      <td>watching the notebook</td>\n",
       "      <td>neutral</td>\n",
       "      <td>watching the notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>895de1648c</td>\n",
       "      <td>really tired. and have to work the whole day tomorrow, t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>really tired.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>78d89e7c64</td>\n",
       "      <td>Yeah prbly pickin up songs for SingStar. Haven`t checke...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah prbly pickin up songs for singstar. haven`t checke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>756d255e40</td>\n",
       "      <td>is at home with a pukey boy! Poor little baby</td>\n",
       "      <td>negative</td>\n",
       "      <td>poor little baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>d4aa6a714a</td>\n",
       "      <td>Sitting in a shadow of the tree in the heart of the city...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks,wind,for being so pleasant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>c9a52dee1f</td>\n",
       "      <td>Guess I`m gonna try the nap thing again 2day, but since ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>since my kids haven`t cooperated with it yet this week ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>99fdaff40d</td>\n",
       "      <td>http://twitpic.com/4wi9p - playing with ethan. i love yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>337ebe2747</td>\n",
       "      <td>FRIDAY so freakin happy today was an annoying day  buuut...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>6816923abd</td>\n",
       "      <td>This month was a bad month to try and get an advert toge...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>982b1c05d3</td>\n",
       "      <td>LOVE the album guys and can`t wait for the official rel...</td>\n",
       "      <td>positive</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>fc5de3287d</td>\n",
       "      <td>: nooo  i don`t know why...i click on TweetDeck_0_25_ma...</td>\n",
       "      <td>negative</td>\n",
       "      <td>so sad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>f79609f1b3</td>\n",
       "      <td>Lovin` , , &amp; _Bailon SOOOO much right now!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>lovin` , , &amp; _bailon soooo much right now!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>af0778ed50</td>\n",
       "      <td>should not have waled past the Quad! I want to be outsid...</td>\n",
       "      <td>positive</td>\n",
       "      <td>fun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>49c713c76d</td>\n",
       "      <td>Is It The Bit Where Hollie Started Crying?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is it the bit where hollie started crying?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>e5a26fb323</td>\n",
       "      <td>lol! woow okay its not that big of a deal</td>\n",
       "      <td>neutral</td>\n",
       "      <td>lol! woow okay its not that big of a deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1086c26e82</td>\n",
       "      <td>http://twitpic.com/4vuuy - That`s so cool.</td>\n",
       "      <td>positive</td>\n",
       "      <td>that`s so cool.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>1f9019bba5</td>\n",
       "      <td>lmao you witty wacko...loves it</td>\n",
       "      <td>positive</td>\n",
       "      <td>loves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>a267c3e874</td>\n",
       "      <td>We are at alexander. Just had a 3 course dinner and i a...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>we are at alexander. just had a 3 course dinner and i a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>ea1b1fd1bb</td>\n",
       "      <td>Mudweight hauled in for last time by   http://yfrog.com/...</td>\n",
       "      <td>negative</td>\n",
       "      <td>mudweight hauled in for last time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "1191  12005b65fc                    Waiting for my turn on wii fit gym closed   \n",
       "661   bcf13877f7                                        Good morning everyone   \n",
       "1968  575e4a89fe                                tts ridiculously sweet of you   \n",
       "826   a0b1828b67  'Brides a la mode' pow wow first thing this morning   Th...   \n",
       "992   472c3e2c41  Getting somewhere with my first 'real' KiokuDB and catal...   \n",
       "1922  ce71d002ec  Mommas day is may 10th! Don`t forget to do something nic...   \n",
       "1043  8db4aaef4a                                        watching the notebook   \n",
       "2111  895de1648c  really tired. and have to work the whole day tomorrow, t...   \n",
       "1695  78d89e7c64   Yeah prbly pickin up songs for SingStar. Haven`t checke...   \n",
       "2220  756d255e40                is at home with a pukey boy! Poor little baby   \n",
       "2743  d4aa6a714a  Sitting in a shadow of the tree in the heart of the city...   \n",
       "2290  c9a52dee1f  Guess I`m gonna try the nap thing again 2day, but since ...   \n",
       "3166  99fdaff40d  http://twitpic.com/4wi9p - playing with ethan. i love yo...   \n",
       "1277  337ebe2747  FRIDAY so freakin happy today was an annoying day  buuut...   \n",
       "1257  6816923abd  This month was a bad month to try and get an advert toge...   \n",
       "2241  982b1c05d3   LOVE the album guys and can`t wait for the official rel...   \n",
       "2343  fc5de3287d   : nooo  i don`t know why...i click on TweetDeck_0_25_ma...   \n",
       "2892  f79609f1b3                   Lovin` , , & _Bailon SOOOO much right now!   \n",
       "254   af0778ed50  should not have waled past the Quad! I want to be outsid...   \n",
       "2418  49c713c76d                   Is It The Bit Where Hollie Started Crying?   \n",
       "867   e5a26fb323                    lol! woow okay its not that big of a deal   \n",
       "309   1086c26e82                   http://twitpic.com/4vuuy - That`s so cool.   \n",
       "1472  1f9019bba5                              lmao you witty wacko...loves it   \n",
       "1930  a267c3e874   We are at alexander. Just had a 3 course dinner and i a...   \n",
       "1243  ea1b1fd1bb  Mudweight hauled in for last time by   http://yfrog.com/...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "1191   neutral                    waiting for my turn on wii fit gym closed  \n",
       "661   positive                                                         good  \n",
       "1968  positive                                    ridiculously sweet of you  \n",
       "826   positive                                                       lovely  \n",
       "992    neutral   getting somewhere with my first 'real' kiokudb and cata...  \n",
       "1922  positive                                                         nice  \n",
       "1043   neutral                                        watching the notebook  \n",
       "2111  negative                                                really tired.  \n",
       "1695   neutral   yeah prbly pickin up songs for singstar. haven`t checke...  \n",
       "2220  negative                                             poor little baby  \n",
       "2743  positive                            thanks,wind,for being so pleasant  \n",
       "2290  negative   since my kids haven`t cooperated with it yet this week ...  \n",
       "3166  positive                                                       i love  \n",
       "1277  positive                                                        happy  \n",
       "1257  negative                                                          bad  \n",
       "2241  positive                                                         love  \n",
       "2343  negative                                                    so sad...  \n",
       "2892   neutral                   lovin` , , & _bailon soooo much right now!  \n",
       "254   positive                                                         fun.  \n",
       "2418   neutral                   is it the bit where hollie started crying?  \n",
       "867    neutral                    lol! woow okay its not that big of a deal  \n",
       "309   positive                                              that`s so cool.  \n",
       "1472  positive                                                        loves  \n",
       "1930   neutral   we are at alexander. just had a 3 course dinner and i a...  \n",
       "1243  negative                            mudweight hauled in for last time  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
